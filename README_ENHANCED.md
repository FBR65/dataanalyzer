# ğŸš€ Enhanced Data Analyzer - Excel zu DuckDB/DuckLake Transformation

Eine erweiterte Version des Data Analyzers mit automatisierter Excel-zu-DuckDB/DuckLake-Transformation und PyWalker-Integration fÃ¼r Tableau-Ã¤hnliche Datenexploration.

## âœ¨ Neue Features

### ğŸ“Š Excel-zu-DuckDB/DuckLake Pipeline
- **Automatisierte Excel-Verarbeitung**: Bulk-Import von Excel-Dateien
- **DuckDB Integration**: Hochperformante spaltenorientierte Datenbank
- **DuckLake Support**: Parquet-basierter Data Lake mit DuckDB-Integration
- **Intelligente Datenbereinigung**: Automatische Spaltennamensbereinigung und Datentyperkennung
- **Metadaten-Tracking**: VollstÃ¤ndige Nachverfolgung verarbeiteter Dateien

### ğŸ¯ PyWalker Integration
- **Tableau-Ã¤hnliche Exploration**: Drag-and-Drop Datenexploration
- **Interaktive Visualisierungen**: Erweiterte Chart-Typen und KonfigurationsmÃ¶glichkeiten
- **Explorative Datenanalyse**: Intuitive BenutzeroberflÃ¤che fÃ¼r GeschÃ¤ftsanwender

### ğŸ”§ Erweiterte Analysefunktionen
- **Automatische Insights**: KI-gestÃ¼tzte Datenanalyse und Empfehlungen
- **Erweiterte Visualisierungen**: Plotly-basierte interaktive Charts
- **Flexible Datenexporte**: Multiple Formate (CSV, Excel, Parquet, JSON)
- **Performance-optimierte Abfragen**: DuckDB-optimierte SQL-Queries

## ğŸ—ï¸ Architektur

```
Enhanced Data Analyzer/
â”œâ”€â”€ ğŸ“ Excel Processing Layer
â”‚   â”œâ”€â”€ ExcelProcessor (tools/excel_processor.py)
â”‚   â”œâ”€â”€ Automatische Datenerkennung
â”‚   â””â”€â”€ Batch-Verarbeitung
â”œâ”€â”€ ğŸ—„ï¸ DuckDB/DuckLake Layer
â”‚   â”œâ”€â”€ DuckDBConnector (tools/duckdb_connector.py)
â”‚   â”œâ”€â”€ Parquet-basierte Speicherung
â”‚   â””â”€â”€ Erweiterte SQL-Funktionen
â”œâ”€â”€ ğŸ¨ Frontend Layer
â”‚   â”œâ”€â”€ Enhanced Gradio Interface
â”‚   â”œâ”€â”€ PyWalker Integration
â”‚   â””â”€â”€ Erweiterte Visualisierungen
â””â”€â”€ ğŸ³ Containerization
    â”œâ”€â”€ Docker + Docker Compose
    â”œâ”€â”€ Ollama LLM Support
    â””â”€â”€ Optionale Services (MongoDB, MariaDB, Jupyter, Metabase)
```

## ğŸš€ Schnellstart

### Option 1: Docker (Empfohlen)
```bash
# Repository klonen
git clone <repository-url>
cd dataanalyzer

# Umgebungsvariablen setzen (optional)
cp .env.example .env

# Services starten
docker-compose up -d

# Warten bis alle Services bereit sind
docker-compose logs -f dataanalyzer
```

**Zugriff auf Services:**
- **Data Analyzer**: http://localhost:7860
- **Jupyter Notebook**: http://localhost:8888 (Token: dataanalyzer123)
- **Metabase**: http://localhost:3000
- **Ollama**: http://localhost:11434

### Option 2: Lokale Installation
```bash
# AbhÃ¤ngigkeiten installieren
uv sync

# Verzeichnisse erstellen
mkdir -p excel_files databases ducklake generated_code logs

# Enhanced Frontend starten
uv run python enhanced_frontend.py
```

## ğŸ’¡ Verwendung

### 1. Excel-Dateien verarbeiten
```python
from tools.excel_processor import ExcelProcessor

# Processor initialisieren
processor = ExcelProcessor(
    source_dir="./excel_files",
    duck_lake_path="./ducklake",
    duckdb_path="./databases/excel_data.duckdb"
)

# Alle Excel-Dateien verarbeiten
results = processor.process_directory(recursive=True)
print(f"Verarbeitet: {results['processed']} Dateien")
```

### 2. Daten mit DuckDB abfragen
```python
from tools.duckdb_connector import DuckDBConnector

# Connector initialisieren
connector = DuckDBConnector(
    duckdb_path="./databases/excel_data.duckdb",
    duck_lake_path="./ducklake"
)

# VerfÃ¼gbare Tabellen anzeigen
tables = connector.list_tables()
print(f"VerfÃ¼gbare Tabellen: {[t['table_name'] for t in tables]}")

# Daten abfragen
df = connector.execute_query("SELECT * FROM my_table LIMIT 100")
print(df.head())
```

### 3. PyWalker fÃ¼r Exploration verwenden
```python
import pygwalker as pyg

# PyWalker Interface erstellen
pyg_html = pyg.to_html(df)
# HTML wird in Gradio Interface eingebettet
```

## ğŸ¯ Workflow-Beispiele

### Beispiel 1: Automatisierte Verkaufsdaten-Analyse
```python
# 1. Excel-Verkaufsdaten verarbeiten
processor = ExcelProcessor("./sales_data")
results = processor.process_directory()

# 2. Daten analysieren
connector = DuckDBConnector("databases/sales.duckdb")
sales_summary = connector.execute_query("""
    SELECT 
        region,
        SUM(sales_amount) as total_sales,
        COUNT(*) as transactions
    FROM sales_data 
    GROUP BY region
    ORDER BY total_sales DESC
""")

# 3. Insights generieren
insights = connector.generate_insights("sales_data")
```

### Beispiel 2: Multi-File Dashboard
```python
# Mehrere Excel-Dateien zu einem Dashboard kombinieren
files = ["customers.xlsx", "orders.xlsx", "products.xlsx"]

for file in files:
    processor.process_excel_file(f"./data/{file}")

# Joined Analysis
combined_data = connector.execute_query("""
    SELECT 
        c.customer_name,
        o.order_date,
        p.product_name,
        o.quantity * p.price as revenue
    FROM customers c
    JOIN orders o ON c.customer_id = o.customer_id
    JOIN products p ON o.product_id = p.product_id
""")
```

## ğŸ”§ Konfiguration

### Umgebungsvariablen
```bash
# LLM-Konfiguration
OPENAI_API_KEY=ollama
OPENAI_API_BASE_URL=http://localhost:11434/v1
LLM_MODEL=qwen2.5:latest
CODING_MODEL=qwen2.5-coder:latest

# Datenbank-Konfiguration
DUCKDB_PATH=./databases/main.duckdb
DUCK_LAKE_PATH=./ducklake

# Interface-Konfiguration
GRADIO_SERVER_NAME=0.0.0.0
GRADIO_SERVER_PORT=7860
```

### Excel-Verarbeitung konfigurieren
```python
processor = ExcelProcessor(
    source_dir="./excel_files",
    duck_lake_path="./ducklake",
    use_parquet=True,        # Parquet fÃ¼r bessere Performance
    auto_clean=True,         # Automatische Datenbereinigung
    max_file_size_mb=100     # Maximale DateigrÃ¶ÃŸe
)
```

## ğŸ¨ PyWalker Features

### Drag-and-Drop Exploration
- **Dimensionen**: Ziehen Sie kategorische Felder in Zeilen/Spalten
- **Kennzahlen**: Ziehen Sie numerische Felder in Werte
- **Filter**: Interaktive Datenfilterung
- **Chart-Typen**: Automatische Visualisierungsempfehlungen

### Erweiterte Visualisierungen
- **Heatmaps**: Korrelationsanalysen
- **Geographische Karten**: Standortbasierte Analysen
- **Zeitreihen**: Trend-Analysen
- **Multi-dimensionale Analyse**: Komplexe ZusammenhÃ¤nge

## ğŸ“Š Performance-Optimierungen

### DuckDB-Optimierungen
```sql
-- Indizierte Spalten fÃ¼r bessere Performance
CREATE INDEX idx_date ON sales_data(order_date);
CREATE INDEX idx_customer ON sales_data(customer_id);

-- Partitionierte Tabellen
CREATE TABLE sales_partitioned AS 
SELECT * FROM sales_data 
PARTITION BY RANGE(order_date);
```

### Parquet-Optimierungen
```python
# Komprimierte Parquet-Dateien
df.to_parquet(
    "data.parquet",
    compression='snappy',
    row_group_size=10000
)

# Spaltenweise Komprimierung
df.to_parquet(
    "data.parquet",
    compression={'name': 'gzip', 'amount': 'snappy'}
)
```

## ğŸ³ Docker Services

### VollstÃ¤ndige Umgebung
```bash
# Alle Services starten
docker-compose up -d

# Nur Data Analyzer
docker-compose up -d dataanalyzer

# Mit zusÃ¤tzlichen Services
docker-compose up -d dataanalyzer ollama mongodb jupyter metabase
```

### Service-Ãœbersicht
- **dataanalyzer**: Hauptanwendung (Port 7860)
- **ollama**: Lokaler LLM-Server (Port 11434)
- **mongodb**: Dokumentendatenbank (Port 27017)
- **mariadb**: Relationale Datenbank (Port 3306)
- **jupyter**: Notebook-Umgebung (Port 8888)
- **metabase**: BI-Dashboard (Port 3000)

## ğŸ” Troubleshooting

### HÃ¤ufige Probleme

**Excel-Dateien kÃ¶nnen nicht gelesen werden:**
```python
# Verschiedene Engines probieren
df = pd.read_excel(file_path, engine='openpyxl')  # FÃ¼r .xlsx
df = pd.read_excel(file_path, engine='xlrd')      # FÃ¼r .xls
```

**DuckDB-Verbindungsfehler:**
```python
# Verbindung prÃ¼fen
try:
    conn = duckdb.connect("test.duckdb")
    conn.execute("SELECT 1")
    print("âœ… DuckDB verfÃ¼gbar")
except Exception as e:
    print(f"âŒ DuckDB-Fehler: {e}")
```

**PyWalker-Probleme:**
```bash
# PyWalker neu installieren
pip install --upgrade pygwalker
```

## ğŸš€ Roadmap

### Geplante Features
- [ ] **Automatisierte Scheduler**: Cron-basierte Excel-Verarbeitung
- [ ] **Erweiterte BI-Integration**: Tableau/Power BI Connectors
- [ ] **Machine Learning Pipeline**: Automated ML auf DuckDB-Daten
- [ ] **Real-time Processing**: Streaming Excel-Updates
- [ ] **Cloud-Integration**: AWS S3/Azure Blob Storage
- [ ] **REST API**: Programmatischer Zugriff auf alle Funktionen

### Verbesserungen
- [ ] **Performance**: Parallel Excel-Verarbeitung
- [ ] **Skalierbarkeit**: Distributed DuckDB
- [ ] **Monitoring**: Detaillierte Metriken und Alerts
- [ ] **Security**: VerschlÃ¼sselung und Zugriffskontrolle

## ğŸ“„ Lizenz

GNU AFFERO GENERAL PUBLIC LICENSE Version 3

## ğŸ¤ Beitragen

BeitrÃ¤ge sind willkommen! Bitte lesen Sie die CONTRIBUTING.md fÃ¼r Details.

## ğŸ“ Support

Bei Fragen oder Problemen Ã¶ffnen Sie bitte ein Issue auf GitHub.

---

*Powered by DuckDB ğŸ¦†, PyWalker ğŸš¶, Gradio ğŸ¨, und Docker ğŸ³*
